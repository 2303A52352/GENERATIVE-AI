{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMD+P0r+WX8vzoprlL7DHYQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52352/GENERATIVE-AI/blob/main/GENERATIVE-AI%20WEEK3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "WEEK - 3\n",
        "\n",
        "Q1: (1 ponto) Write Python code without using any libraries to find the value of x at which the function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm. f(x) = 5x 4 + 3x 2 + 10"
      ],
      "metadata": {
        "id": "BlOy1wsPAesZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def f_prime(x):\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    for _ in range(num_iterations):\n",
        "        grad = f_prime(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "initial_x = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
        "print(\"The value of x at which f(x) has a minimum is:\", min_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOFaCAU3BHpp",
        "outputId": "1a04624c-152b-4a73-e71e-4fd189f2ce5a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which f(x) has a minimum is: 4.810419162406747e-28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2: (1 ponto) Write Python code without using any libraries to find the value of x and y at which the function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm. f(x) = 3x 2 + 5e −y + 10"
      ],
      "metadata": {
        "id": "W7cBDIS6BPqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def partial_derivative_x(x, y):\n",
        "    return 6 * x\n",
        "\n",
        "def partial_derivative_y(x, y):\n",
        "    return -5 * (2.71828 ** -y)\n",
        "\n",
        "def gradient_descent(initial_x, initial_y, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    y = initial_y\n",
        "    for _ in range(num_iterations):\n",
        "        grad_x = partial_derivative_x(x, y)\n",
        "        grad_y = partial_derivative_y(x, y)\n",
        "        x = x - learning_rate * grad_x\n",
        "        y = y - learning_rate * grad_y\n",
        "    return x, y\n",
        "initial_x = 0.5\n",
        "initial_y = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x, min_y = gradient_descent(initial_x, initial_y, learning_rate, num_iterations)\n",
        "print(\"The values of x and y at which g(x, y) has a minimum are:\", min_x, min_y)\n",
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zh6dQED9BQUf",
        "outputId": "738251d3-0a9b-4601-c622-843d7cf0a4e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values of x and y at which g(x, y) has a minimum are: 6.711561962466847e-28 3.946138890954553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3: (1 ponto) Write Python code without using any libraries to find the value of x at which the sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent Algorithm. z(x) = 1 1 + e −x"
      ],
      "metadata": {
        "id": "AaX83D4fBT3v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return 1 / (1 + 2.71828 ** -x)\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "    sig = sigmoid(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "\n",
        "def gradient_descent(initial_x, learning_rate, num_iterations):\n",
        "    x = initial_x\n",
        "    for _ in range(num_iterations):\n",
        "        grad = sigmoid_derivative(x)\n",
        "        x = x - learning_rate * grad\n",
        "    return x\n",
        "\n",
        "initial_x = 0.5\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "min_x = gradient_descent(initial_x, learning_rate, num_iterations)\n",
        "print(\"The value of x at which z(x) has a minimum is:\", min_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LeU810kcBWu3",
        "outputId": "701145b0-0e57-4a6d-8b71-5280d70c6fdf"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x at which z(x) has a minimum is: -1.6012961162961212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4: (1 ponto) Write Python code without using any libraries to find the value of optimal values of model parameters M and C such that the model’s Square Error Value shown in equation 4 will be minimum. It means model gives output close to expected output as shown in SE = (ExpectedOutput − P redictedOutput) 2"
      ],
      "metadata": {
        "id": "gH1J8ls4Bb7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_gradients(x, y_true, M, C):\n",
        "    y_pred = M * x + C\n",
        "    error = y_true - y_pred\n",
        "    gradient_M = -2 * x * error\n",
        "    gradient_C = -2 * error\n",
        "    return gradient_M, gradient_C\n",
        "\n",
        "\n",
        "def gradient_descent(x, y_true, initial_M, initial_C, learning_rate, num_iterations):\n",
        "    M = initial_M\n",
        "    C = initial_C\n",
        "    for _ in range(num_iterations):\n",
        "        total_gradient_M = 0\n",
        "        total_gradient_C = 0\n",
        "        for i in range(len(x)):\n",
        "            grad_M, grad_C = compute_gradients(x[i], y_true[i], M, C)\n",
        "            total_gradient_M += grad_M\n",
        "            total_gradient_C += grad_C\n",
        "        M = M - learning_rate * (total_gradient_M / len(x))\n",
        "        C = C - learning_rate * (total_gradient_C / len(x))\n",
        "    return M, C\n",
        "\n",
        "\n",
        "x = [1, 2, 3, 4, 5]\n",
        "y_true = [3, 7, 11, 15, 19]\n",
        "initial_M = 0.0\n",
        "initial_C = 0.0\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "optimal_M, optimal_C = gradient_descent(x, y_true, initial_M, initial_C, learning_rate, num_iterations)\n",
        "print(\"The optimal values of M and C are:\", optimal_M, optimal_C)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "scbfgo6MBl35",
        "outputId": "27a507c9-2a02-4500-f7ca-04001a881b12"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The optimal values of M and C are: 3.981660469673651 -0.9337884764204182\n"
          ]
        }
      ]
    }
  ]
}